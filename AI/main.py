# main.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import numpy as np
import pandas as pd
import asyncio
import logging
from aiokafka import AIOKafkaConsumer, AIOKafkaProducer
from sklearn.preprocessing import StandardScaler

from model_loader import sleephony, LABELS

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("sleephony")

KAFKA_BOOTSTRAP = "k12c208.p.ssafy.io:29092"
REQUEST_TOPIC  = "sleep-stage-raw-request"
RESPONSE_TOPIC = "sleep-stage-raw-response"
GROUP_ID       = "sleepony-fastapi-group"
app = FastAPI(title="Sleephony API", version="0.1.0", root_path="/ai")

class RawPayload(BaseModel):
    acc_x: List[float]
    acc_y: List[float]
    acc_z: List[float]
    temp:  List[float]
    hr:    List[float]

class RawResponse(BaseModel):
    requestId: str
    labels:    List[str]

producer: AIOKafkaProducer = None
consumer: AIOKafkaConsumer = None

class FeaturePayload(BaseModel):
    # ê¸°ì¡´ predict: ì´ë¯¸ [batch, seq_len=5, feat_dim=11] í˜•íƒœì˜ í”¼ì²˜
    features: List[List[List[float]]]


class RawPayload(BaseModel):
    # ìƒˆë¡œ ì¶”ê°€ëœ predict_raw: ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„°
    acc_x: List[float]
    acc_y: List[float]
    acc_z: List[float]
    temp: List[float]
    hr: List[float]


@app.get("/health")
def health():
    return {"status": "ok"}

# -- FastAPI ì´ë²¤íŠ¸ í›…: startup/shutdown ---------------------------
@app.on_event("startup")
async def startup_event():
    global producer, consumer

    # 1) Kafka Producer ì´ˆê¸°í™”
    producer = AIOKafkaProducer(
        bootstrap_servers=KAFKA_BOOTSTRAP,
        value_serializer=lambda v: v.json().encode("utf-8")
    )
    await producer.start()

    # 2) Kafka Consumer ì´ˆê¸°í™”
    consumer = AIOKafkaConsumer(
        REQUEST_TOPIC,
        bootstrap_servers=KAFKA_BOOTSTRAP,
        group_id=GROUP_ID,
        value_deserializer=lambda b: RawPayload.parse_raw(b)
    )
    await consumer.start()

    # 3) ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ë©”ì‹œì§€ ì²˜ë¦¬ ë£¨í”„ ì‹¤í–‰
    asyncio.create_task(process_loop())


@app.on_event("shutdown")
async def shutdown_event():
    # ì•± ì¢…ë£Œ ì‹œ Kafka ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    await consumer.stop()
    await producer.stop()


# -- ë©”ì‹œì§€ ì²˜ë¦¬ ë£¨í”„ -----------------------------------------
async def process_loop():
    """
    1) REQUEST_TOPICì—ì„œ RawPayload ìˆ˜ì‹ 
    2) ê¸°ì¡´ predict_raw ë¡œì§ ìˆ˜í–‰ â†’ labels ë¦¬ìŠ¤íŠ¸ ìƒì„±
    3) RESPONSE_TOPICìœ¼ë¡œ RawResponse ì „ì†¡ (í—¤ë”ì— requestId í¬í•¨)
    """
    logger.info("[ğŸ”„] Kafka consumer listening for messages...")

    async for msg in consumer:
        payload: RawPayload = msg.value
        headers = dict((k, v.decode()) for k, v in msg.headers)
        request_id = headers.get("requestId", "")

        logger.info("[ğŸ“©] Received message from Kafka")
        logger.info(f"[ğŸ”–] requestId: {request_id}, header keys: {list(headers.keys())}")

        # ë°ì´í„° ê¸¸ì´ ì²´í¬
        df = pd.DataFrame({
            "ACC_X": payload.acc_x,
            "ACC_Y": payload.acc_y,
            "ACC_Z": payload.acc_z,
            "TEMP":  payload.temp,
            "HR":    payload.hr
        }).dropna()
        logger.info(f"[ğŸ“Š] Received {len(df)} samples")

        if len(df) < 30 * 64:
            logger.warning(f"[âš ï¸] Too few samples ({len(df)}), skipping.")

            # ë„ˆë¬´ ì§§ìœ¼ë©´ ë¬´ì‹œí•˜ê±°ë‚˜, Nack ë¡œì§ì„ ì¶”ê°€í•´ë„ ë©ë‹ˆë‹¤.
            continue

        # 1. ACC_MAG ê³„ì‚°
        df["ACC_MAG"] = np.sqrt(df.ACC_X**2 + df.ACC_Y**2 + df.ACC_Z**2)

        # 2. ìœˆë„ì‰ & í”¼ì²˜ ì¶”ì¶œ
        epoch, step = 30 * 64, 15 * 64
        feats = []
        for start in range(0, len(df) - epoch + 1, step):
            seg = df.iloc[start : start + epoch]
            acc = seg.ACC_MAG.values
            tmp = seg.TEMP.values
            hr  = seg.HR.values

            basic = [acc.mean(), acc.std(), tmp.mean(), tmp.std(), hr.mean(), hr.std()]
            ibi   = 60.0 / hr
            rmssd = np.sqrt(np.mean(np.diff(ibi)**2))
            sdnn  = np.std(ibi)
            freqs = np.fft.rfftfreq(len(acc), d=1/64)
            psd   = np.abs(np.fft.rfft(acc))**2
            delta = psd[(freqs>=0.5)&(freqs<4 )].sum()
            theta = psd[(freqs>=4  )&(freqs<8 )].sum()
            alpha = psd[(freqs>=8  )&(freqs<12)].sum()
            feats.append(basic + [rmssd, sdnn, delta, theta, alpha])

        feats = np.array(feats, dtype=np.float32)
        logger.info(f"[ğŸ“] Extracted {len(feats)} feature vectors")

        if feats.shape[0] < 5:
            logger.warning(f"[âš ï¸] Not enough epochs ({feats.shape[0]}) to create sequences.")

            continue

        # 3. subject-wise ì •ê·œí™” + seq_len=5 ì‹œí€€ìŠ¤ ìƒì„±
        scaler = StandardScaler()
        feats  = scaler.fit_transform(feats)
        seqs   = [feats[i:i+5] for i in range(len(feats)-4)]
        arr    = np.stack(seqs)  # shape = [n_seq, 5, 11]
        logger.info(f"[ğŸ¤–] Running inference on {arr.shape[0]} sequences")

        # 4. ëª¨ë¸ ì˜ˆì¸¡
        preds = sleephony.predict(arr)
        idxs  = np.argmax(preds, axis=1)
        labels = [LABELS[int(i)] for i in idxs]
        logger.info(f"[âœ…] Predicted labels: {labels[:5]}{'...' if len(labels) > 5 else ''}")

        # 5. ì‘ë‹µ ì „ì†¡
        response = RawResponse(requestId=request_id, labels=labels)
        await producer.send_and_wait(
            RESPONSE_TOPIC,
            response,
            headers=[("requestId", request_id.encode("utf-8"))]
        )
        logger.info(f"[ğŸ“¤] Response sent to Kafka with requestId: {request_id}")
    

@app.post("/api/ai/sleep-stage")
def predict_raw(payload: RawPayload):
    """
    ì›ë³¸ ì‹œê³„ì—´(raw ACC_X,Y,Z / TEMP / HR)ì„ ë°›ì•„
    1) ì—í¬í¬(30s, 64Hz) ë‹¨ìœ„ë¡œ ê²¹ì¹˜ëŠ” ìœˆë„ìš° â†’ í”¼ì²˜(11-dim) ì¶”ì¶œ
    2) subject-wise ì •ê·œí™”
    3) seq_len=5 ë‹¨ìœ„ë¡œ ì‹œí€€ìŠ¤ ìœˆë„ì‰
    4) ëª¨ë¸ ì˜ˆì¸¡ â†’ ë¼ë²¨ ë°˜í™˜
    """
    # 1. DataFrame ìƒì„± & ê¸°ë³¸ ê²€ì¦
    df = pd.DataFrame({
        "ACC_X": payload.acc_x,
        "ACC_Y": payload.acc_y,
        "ACC_Z": payload.acc_z,
        "TEMP": payload.temp,
        "HR": payload.hr
    }).dropna()
    min_samples = 30 * 64  # 30ì´ˆ * 64Hz
    if len(df) < min_samples:
        raise HTTPException(400, f"ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ {min_samples} ìƒ˜í”Œ í•„ìš”")

    # 2. ê°€ì†ë„ ë²¡í„° í¬ê¸°
    df["ACC_MAG"] = np.sqrt(df.ACC_X**2 + df.ACC_Y**2 + df.ACC_Z**2)

    # 3. ìœˆë„ì‰ & í”¼ì²˜ ì¶”ì¶œ
    epoch_size = 30 * 64
    step       = 15 * 64
    feats = []
    for start in range(0, len(df) - epoch_size + 1, step):
        seg = df.iloc[start : start + epoch_size]
        acc = seg.ACC_MAG.values
        tmp = seg.TEMP.values
        hr  = seg.HR.values

        # ê¸°ë³¸ í†µê³„
        f_basic = [
            acc.mean(), acc.std(),
            tmp.mean(), tmp.std(),
            hr.mean(),  hr.std()
        ]
        # HRV
        ibi   = 60.0 / hr
        rmssd = np.sqrt(np.mean(np.diff(ibi)**2))
        sdnn  = np.std(ibi)
        # FFT ë°´ë“œíŒŒì›Œ
        freqs = np.fft.rfftfreq(len(acc), d=1/64)
        psd   = np.abs(np.fft.rfft(acc))**2
        delta = psd[(freqs>=0.5)&(freqs<4 )].sum()
        theta = psd[(freqs>=4  )&(freqs<8 )].sum()
        alpha = psd[(freqs>=8  )&(freqs<12)].sum()

        feats.append(f_basic + [rmssd, sdnn, delta, theta, alpha])

    feats = np.array(feats, dtype=np.float32)
    if feats.shape[0] < 5:
        raise HTTPException(400, "ì—í¬í¬ ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ seq_len=5ë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # 4. subject-wise ì •ê·œí™”
    scaler = StandardScaler()
    feats  = scaler.fit_transform(feats)

    # 5. ì‹œí€€ìŠ¤ ìœˆë„ì‰ (seq_len=5)
    seq_len = 5
    seqs = []
    for i in range(feats.shape[0] - seq_len + 1):
        seqs.append(feats[i : i + seq_len])
    arr = np.stack(seqs)  # shape = [n_seq, 5, 11]

    # 6. ì˜ˆì¸¡
    preds = sleephony.predict(arr)
    idxs = np.argmax(preds, axis=1)
    labels = [LABELS[int(i)] for i in idxs]

    return {"labels": labels}
