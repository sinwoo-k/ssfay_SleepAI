# # main.py

# from fastapi import FastAPI, HTTPException
# from pydantic import BaseModel
# from typing import List
# import numpy as np
# import pandas as pd
# import asyncio
# import logging
# from aiokafka import AIOKafkaConsumer, AIOKafkaProducer
# from sklearn.preprocessing import StandardScaler

# from model_loader import sleephony, LABELS

# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger("sleephony")

# KAFKA_BOOTSTRAP = "k12c208.p.ssafy.io:29092"
# REQUEST_TOPIC  = "sleep-stage-raw-request"
# RESPONSE_TOPIC = "sleep-stage-raw-response"
# GROUP_ID       = "sleepony-fastapi-group"
# app = FastAPI(title="Sleephony API", version="0.1.0", root_path="/ai")

# # 20Hz ê¸°ì¤€ íŒŒë¼ë¯¸í„°
# SAMPLING_RATE = 20
# EPOCH_SECONDS = 30
# STEP_SECONDS  = 15
# SEQ_LEN       = 5

# EPOCH_SIZE = EPOCH_SECONDS * SAMPLING_RATE
# STEP       = STEP_SECONDS  * SAMPLING_RATE

# class RawPayload(BaseModel):
#     acc_x: List[float]
#     acc_y: List[float]
#     acc_z: List[float]
#     temp:  List[float]
#     hr:    List[float]

# class RawResponse(BaseModel):
#     requestId: str
#     labels:    List[str]

# producer: AIOKafkaProducer = None
# consumer: AIOKafkaConsumer = None

# class FeaturePayload(BaseModel):
#     # ê¸°ì¡´ predict: ì´ë¯¸ [batch, seq_len=5, feat_dim=11] í˜•íƒœì˜ í”¼ì²˜
#     features: List[List[List[float]]]


# class RawPayload(BaseModel):
#     # ìƒˆë¡œ ì¶”ê°€ëœ predict_raw: ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„°
#     acc_x: List[float]
#     acc_y: List[float]
#     acc_z: List[float]
#     temp: List[float]
#     hr: List[float]


# @app.get("/health")
# def health():
#     return {"status": "ok"}

# # -- FastAPI ì´ë²¤íŠ¸ í›…: startup/shutdown ---------------------------
# @app.on_event("startup")
# async def startup_event():
#     global producer, consumer

#     # 1) Kafka Producer ì´ˆê¸°í™”
#     producer = AIOKafkaProducer(
#         bootstrap_servers=KAFKA_BOOTSTRAP,
#         value_serializer=lambda v: v.json().encode("utf-8")
#     )
#     await producer.start()

#     # 2) Kafka Consumer ì´ˆê¸°í™”
#     consumer = AIOKafkaConsumer(
#         REQUEST_TOPIC,
#         bootstrap_servers=KAFKA_BOOTSTRAP,
#         group_id=GROUP_ID,
#         value_deserializer=lambda b: RawPayload.parse_raw(b)
#     )
#     await consumer.start()

#     # 3) ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ë©”ì‹œì§€ ì²˜ë¦¬ ë£¨í”„ ì‹¤í–‰
#     asyncio.create_task(process_loop())


# @app.on_event("shutdown")
# async def shutdown_event():
#     # ì•± ì¢…ë£Œ ì‹œ Kafka ë¦¬ì†ŒìŠ¤ ì •ë¦¬
#     await consumer.stop()
#     await producer.stop()


# # -- ë©”ì‹œì§€ ì²˜ë¦¬ ë£¨í”„ -----------------------------------------
# async def process_loop():
#     """
#     1) REQUEST_TOPICì—ì„œ RawPayload ìˆ˜ì‹ 
#     2) ê¸°ì¡´ predict_raw ë¡œì§ ìˆ˜í–‰ â†’ labels ë¦¬ìŠ¤íŠ¸ ìƒì„±
#     3) RESPONSE_TOPICìœ¼ë¡œ RawResponse ì „ì†¡ (í—¤ë”ì— requestId í¬í•¨)
#     """
#     logger.info("[ğŸ”„] Kafka consumer listening for messages...")

#     async for msg in consumer:
#         payload: RawPayload = msg.value
#         headers = dict((k, v.decode()) for k, v in msg.headers)
#         request_id = headers.get("requestId", "")

#         logger.info("[ğŸ“©] Received message from Kafka")
#         logger.info(f"[ğŸ”–] requestId: {request_id}, header keys: {list(headers.keys())}")

#         # 0) DataFrame ìƒì„± & ê¸¸ì´ ì²´í¬
#         df = pd.DataFrame({
#             "ACC_X": payload.acc_x,
#             "ACC_Y": payload.acc_y,
#             "ACC_Z": payload.acc_z,
#             "TEMP":  payload.temp,
#             "HR":    payload.hr
#         }).dropna()
#         logger.info(f"[ğŸ“Š] Received {len(df)} samples")

#         if len(df) < EPOCH_SIZE:
#             logger.warning(f"[âš ï¸] Too few samples ({len(df)}), min required is {EPOCH_SIZE}.")
#             raise HTTPException(
#                 status_code=400,
#                 detail=f"ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ {EPOCH_SIZE} ìƒ˜í”Œ í•„ìš”"
#             )

#         # 1) ACC_MAG ê³„ì‚°
#         df["ACC_MAG"] = np.sqrt(df.ACC_X**2 + df.ACC_Y**2 + df.ACC_Z**2)

#         # 2) ìœˆë„ì‰ & í”¼ì²˜ ì¶”ì¶œ
#         feats = []
#         for start in range(0, len(df) - EPOCH_SIZE + 1, STEP):
#             seg = df.iloc[start : start + EPOCH_SIZE]
#             acc = seg.ACC_MAG.values
#             tmp = seg.TEMP.values
#             hr  = seg.HR.values

#             basic = [
#                 acc.mean(), acc.std(),
#                 tmp.mean(), tmp.std(),
#                 hr.mean(),  hr.std()
#             ]
#             ibi   = 60.0 / hr
#             rmssd = np.sqrt(np.mean(np.diff(ibi)**2))
#             sdnn  = np.std(ibi)

#             freqs = np.fft.rfftfreq(len(acc), d=1/SAMPLING_RATE)
#             psd   = np.abs(np.fft.rfft(acc))**2
#             delta = psd[(freqs>=0.5)&(freqs<4 )].sum()
#             theta = psd[(freqs>=4  )&(freqs<8 )].sum()
#             alpha = psd[(freqs>=8  )&(freqs<12)].sum()

#             feats.append(basic + [rmssd, sdnn, delta, theta, alpha])

#         feats = np.array(feats, dtype=np.float32)
#         logger.info(f"[ğŸ“] Extracted {len(feats)} feature vectors")

#         if feats.shape[0] < SEQ_LEN:
#             logger.warning(f"[âš ï¸] Not enough epochs ({feats.shape[0]}) to create sequences.")
#             raise HTTPException(
#                 status_code=400,
#                 detail=f"ì—í¬í¬ ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ seq_len={SEQ_LEN}ë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
#             )

#         # 3) ì •ê·œí™” & ì‹œí€€ìŠ¤ ìƒì„±
#         scaler = StandardScaler()
#         feats  = scaler.fit_transform(feats)
#         seqs   = [feats[i:i+SEQ_LEN] for i in range(len(feats)-SEQ_LEN+1)]
#         arr    = np.stack(seqs)  # shape=(n_seq, SEQ_LEN, feature_dim)
#         logger.info(f"[ğŸ¤–] Running inference on {arr.shape[0]} sequences")

#         # 4) ëª¨ë¸ ì˜ˆì¸¡
#         preds = sleephony.predict(arr)
#         idxs  = np.argmax(preds, axis=1)
#         labels = [LABELS[int(i)] for i in idxs]
#         logger.info(f"[âœ…] Predicted labels: {labels[:5]}{'...' if len(labels)>5 else ''}")

#         # 5. ì‘ë‹µ ì „ì†¡
#         response = RawResponse(requestId=request_id, labels=labels)
#         await producer.send_and_wait(
#             RESPONSE_TOPIC,
#             response,
#             headers=[("requestId", request_id.encode("utf-8"))]
#         )
#         logger.info(f"[ğŸ“¤] Response sent to Kafka with requestId: {request_id}")
    
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

from model_loader import sleephony, LABELS

app = FastAPI(title="Sleephony API", version="0.1.0")

EPOCH_SECONDS = 30
STEP_SECONDS  = 15
SEQ_LEN       = 5
SAMPLING_RATE = 20  # â† 20Hz ë¡œ ë°ì´í„° ë°›ëŠ”ë‹¤ ê°€ì •

EPOCH_SIZE = EPOCH_SECONDS * SAMPLING_RATE
STEP       = STEP_SECONDS  * SAMPLING_RATE

class RawPayload(BaseModel):
    acc_x: List[float]
    acc_y: List[float]
    acc_z: List[float]
    temp:  List[float]
    hr:    List[float]

@app.get("/health")
def health():
    return {"status": "ok"}

@app.post("/api/ai/sleep-stage")
def predict_raw(payload: RawPayload):
    # 1. ë°ì´í„°í”„ë ˆì„ ìƒì„± & ê¸¸ì´ ì²´í¬
    df = pd.DataFrame({
        "ACC_X": payload.acc_x,
        "ACC_Y": payload.acc_y,
        "ACC_Z": payload.acc_z,
        "TEMP":  payload.temp,
        "HR":    payload.hr
    }).dropna()
    if len(df) < EPOCH_SIZE:
        raise HTTPException(400, f"ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ {EPOCH_SIZE} ìƒ˜í”Œ í•„ìš”")

    # 2. ACC_MAG ê³„ì‚°
    df["ACC_MAG"] = np.sqrt(df.ACC_X**2 + df.ACC_Y**2 + df.ACC_Z**2)

    # 3. ìœˆë„ì‰ & í”¼ì²˜ ì¶”ì¶œ
    feats = []
    for start in range(0, len(df) - EPOCH_SIZE + 1, STEP):
        seg = df.iloc[start : start + EPOCH_SIZE]
        acc = seg.ACC_MAG.values
        tmp = seg.TEMP.values
        hr  = seg.HR.values
        basic = [acc.mean(), acc.std(), tmp.mean(), tmp.std(), hr.mean(), hr.std()]
        ibi   = 60.0 / hr
        rmssd = np.sqrt(np.mean(np.diff(ibi)**2))
        sdnn  = np.std(ibi)
        freqs = np.fft.rfftfreq(len(acc), d=1/SAMPLING_RATE)
        psd   = np.abs(np.fft.rfft(acc))**2
        delta = psd[(freqs>=0.5)&(freqs<4 )].sum()
        theta = psd[(freqs>=4  )&(freqs<8 )].sum()
        alpha = psd[(freqs>=8  )&(freqs<12)].sum()
        feats.append(basic + [rmssd, sdnn, delta, theta, alpha])

    feats = np.array(feats, dtype=np.float32)
    if feats.shape[0] < SEQ_LEN:
        raise HTTPException(400, f"ì—í¬í¬ ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ seq_len={SEQ_LEN}ë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # 4. ì •ê·œí™” & ì‹œí€€ìŠ¤ ìƒì„±
    feats  = StandardScaler().fit_transform(feats)
    seqs   = [feats[i:i+SEQ_LEN] for i in range(len(feats)-SEQ_LEN+1)]
    arr    = np.stack(seqs)  # (n_seq, SEQ_LEN, feat_dim)

    # 5. ì˜ˆì¸¡
    preds = sleephony.predict(arr)
    idxs  = np.argmax(preds, axis=1)
    labels = [LABELS[i] for i in idxs]

    return {"labels": labels}